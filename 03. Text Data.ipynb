{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 텍스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 정규 표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|기호|<center>의미</center>|\n",
    "|:---:|:---|\n",
    "|`*`|바로 앞에 있는 문자, 하위 표현식이 0번 이상 반복됨.|\n",
    "|`+`|바로 앞에 있는 문자, 하위 표현식이 1번 이상 반복됨.|\n",
    "|`[]`|대괄호 안에 있는 문자 중 하나가 나타남.|\n",
    "|`()`|괄호 안의 정규식을 하위 표현식 그룹으로 만듦. <br>정규 표현식을 평가할 때는 하위 표현식이 가장 먼저 평가됨.|\n",
    "|`.`|어떠한 형태든 문자 1자를 나타냄.|\n",
    "|`^`|바로 뒤에 있는 문자, 하위 표현식이 문자열 맨 앞에 나타남.|\n",
    "|`$`|바로 뒤에 있는 문자, 하위 표현식이 문자열 맨 뒤에 나타남.|\n",
    "|`{m}`|바로 뒤에 있는 문자, 하위 표현식이 m회 반복됨.|\n",
    "|`{m,n}`|바로 뒤에 있는 문자, 하위 표현식이 m번 이상, n번 이하 나타남.|\n",
    "|`\\|` | 분리된 문자, 문자열, 하위 표현식 중 하나가 나타남(or와 같은 용법).|\n",
    "|[^]|대괄호 안에 있는 문자를 제외한 문자가 나타남.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "정규표현식은 `re` 라이브러리를 통해 구현할 수 있다.  \n",
    "`compile` : 정규표현식 패턴 지정함수  \n",
    "`findall` : 정규표현식에 부합하는 문자열을 찾는 함수  \n",
    "***  \n",
    "\n",
    "[예제1] a가 1회이상, b가 0회 이상 나오는 문자열을 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaa', 'aabbb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r = re.compile(\"a+b*\")\n",
    "r.findall(\"aaaa, cc, bbbb, aabbb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[예제2] 대문자로 구성된 문자열을 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HOME', 'CAT', 'D', 'THR']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile(\"[A-Z]+\")\n",
    "r.findall(\"HOME, home, CAT, Dog, THRee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[예제3] a로 시작하는 3글자짜리 문자열을 찾아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aDc']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile(\"^a..\") \n",
    "r.findall(\"aDc, cba, acD, abC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[예제4] a가 2회이상 3회이하, b가 2회이상 3회이하인 문자열을 찾아라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aabb', 'aaabb', 'aaabbb']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile(\"a{2,3}b{2,3}\")\n",
    "r.findall(\"aabb, aaabb, ab, aab, aaaaaaaaaaabbbbbbbbbbbbbbb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`search` : compile에 지정된 정규표현식 패턴과 일치하는 문자열의 위치를 반환  \n",
    "`group` : 패턴과 일치하는 문자들을 그룹핑하여 추출  \n",
    "***\n",
    "[예제5] 다음 주소에서 `http:`를 추출하여라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\".+:\")\n",
    "m = p.search(\"http://google.com\")\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`sub(1st, 2nd)` : 정규표현식과 일치하는 부분을 다른 문자로 바꿈.  \n",
    "`1st` : '무엇으로(to)' 바꿀것인가  \n",
    "`2nd` : '무엇을(from)' 바꿀것인가  \n",
    "정규표현식과 일치하는 부분을 삭제하고 싶으면 `1st`에 `\"\"` 입력.\n",
    "***\n",
    "[예제6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그의 물건에 손대지 마시오.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"(내|나의|내꺼)\")\n",
    "p.sub(\"그의\", \"나의 물건에 손대지 마시오.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 사전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "> `corpus(말뭉치)` : 텍스트 마이닝에 적용되는 텍스트 데이터 집합  \n",
    "`정형화` : 원(raw)데이터를 정제하는 작업. 전처리와 동의어\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 대소문자 통일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "파이썬은 대문자와 소문자를 서로 다른 문자로 인식하므로 둘중 하나로 통일해야한다.  \n",
    "대소문자 변환은 `lower()` 또는 `upper()` 매서드를 이용한다.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world HELLO WORLD\n"
     ]
    }
   ],
   "source": [
    "s = \"Hello World\"\n",
    "print(s.lower(), s.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 숫자, 문장부호, 특수문자 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "숫자, 문장부호, 특수문자는 단어가 아니므로 분석의 대상이 아닌 경우가 많다. 하지만 경우에 따라 삭제시 결과가 왜곡된다고 판단되면 남겨둘 수도 있다. 숫자는 `정규 표현식`을 이용해 제거한다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서울 부동산 가격이 올해 들어 평균 % 상승했습니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"[0-9]+\")\n",
    "p.sub(\"\",\"서울 부동산 가격이 올해 들어 평균 30% 상승했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "문장부호, 특수문자 역시 `정규표현식`으로 삭제한다.  \n",
    "\n",
    "> `\\W` : 밑줄을 제외한 모든 특수문자, 문장부호를 가리키는 정규표현식\n",
    "\n",
    "특수문자에는 공백(space)도 포함되므로 문장의 의미를 해치지 않기 위해서 삭제하는 것 보다 공백으로 바꾸는것이 더 좋다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 서울 부동산 가격이 올해 들어 평균 30 상승했습니다 '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"\\W+\")\n",
    "p.sub(\" \",\"★서울 부동산 가격이 올해 들어 평균 30% 상승했습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제_1 건강한 몸과 건강한 정신 '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"\\W+\")\n",
    "s = p.sub(\" \",\"주제_1: 건강한 몸과 건강한 정신!\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "밑줄( _ )은 없애고싶다면 정제가 끝난 뒤 마지막에 한번 더 없앤다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제 1 건강한 몸과 건강한 정신 '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\"_\")\n",
    "p.sub(\" \",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "> `불용어(stopwords)` : 빈번하게 사용되나 구체적인 의미를 찾기 어려운 단어들  ex) a, the, an 등\n",
    "  \n",
    "언어마다 불용어 리스트를 라이브러리에서 제공해주는 경우가 많다. 분석가가 개인적으로 불용어 리스트를 만들어 추가할수도 있다.  \n",
    "  \n",
    "> `NLTK` : 파이썬에서 가장 많이 쓰는 텍스트마이닝 패키지. 언어별로 불용어리스트 제공하나 한국어는 지원하지 않음.  \n",
    "  \n",
    "한국어 불용어 리스트는 아직까지 특정 패키지에서 지원하지 않으므로 개인적으로 작성하거나 다른 분석가들이 작성한 것을 활용해야함.\n",
    "\n",
    "***\n",
    "\n",
    "< 한국어에서 불용어 제거 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추석', '연휴', '민족', '대이동', '시작', '교통량', '교통사고', '자동차', '고장', '상당수', '차지']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_Korean = [\"추석\",\"연휴\",\"민족\",\"대이동\",\"시작\",\"늘어\",\"교통량\",\"교통사고\",\"자동차\",\"고장\",\"상당수\",\"차지\",\"나타\",\"것\",\"기자\"]\n",
    "stopwords = [\"가다\",\"늘어\",\"나타\",\"것\",\"기자\"]\n",
    "[i for i in words_Korean if i not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "< 영어에서 불용어 제거 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', ',', 'president', 'clinton', ',', 'president', 'bush', 'president', 'obama', ',', 'fellow', 'americans', 'people', 'world', 'thank']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "words_English = ['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', ',', 'president', 'clinton', ',', 'president', 'bush', 'president', 'obama', ',', 'fellow', 'americans', 'and', 'people', 'of', 'the', 'world', 'thank', 'you']\n",
    "print([w for w in words_English if not w in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 같은 어근 동일화(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "NLTK 패키지에서 `PorterStemmer` 라이브러리를 통해 어근 통일화를 수행. 그 외에도 `LancasterStemmer`, `RegexpStemmer`도 있음.  \n",
    "  \n",
    "한국어의 경우 stemming을 수행할 수 있는 라이브러리가 없어 형태소 분석기 이용.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is import to be immers while you are python with python . all python have python poorli at least onc . "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps_stemmer = PorterStemmer()\n",
    "\n",
    "new_text = \"It is important to be immersed while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is import to be immers whil you ar python with python . al python hav python poor at least ont . "
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "LS_stemmer = LancasterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(LS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is important to be immersed while you are ing with  . All ers have ed poorly at least once . "
     ]
    }
   ],
   "source": [
    "from nltk.stem.regexp import RegexpStemmer\n",
    "\n",
    "RS_stemmer = RegexpStemmer(\"python\")\n",
    "\n",
    "for w in words:\n",
    "    print(RS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexpStemmer는 특정 표현식을 일괄 제거하는 역할을 한다. 위 코드에서는 python을 제거했다. RegexpStemmer는 Porter나 Lancaster가 처리하지 못하는 특수한 부분에서 쓰는 것이 좋다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `n-gram` : n번 연이어 등장하는 단어들의 연쇄.  \n",
    "바이그램 : 두 번 연이어 등장  \n",
    "트라이그램 : 세 번 연이어 등장  \n",
    "  \n",
    "트라이그램 이상은 잘 활용하지 않는다. 엔그램은 보통 영어에만 적용한다.  \n",
    "\n",
    "바이그램 이상의 엔그램만 독자적으로 활용하는 것은 위험하다. 유니그램(1-gram)과 혼합하여 단어를 도출하는 것이 이상적이다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chief', 'Justice') ('Justice', 'Roberts,') ('Roberts,', 'President') ('President', 'Cater,') ('Cater,', 'President') ('President', 'Clinton,') ('Clinton,', 'President') ('President', 'Bush,') ('Bush,', 'President') ('President', 'Obama,') ('Obama,', 'fellow') ('fellow', 'Americans') ('Americans', 'and') ('and', 'people') ('people', 'of') ('of', 'the') ('the', 'world,') ('world,', 'thank') ('thank', 'you.') ('you.', 'We,') ('We,', 'the') ('the', 'citizens') ('citizens', 'of') ('of', 'America') ('America', 'are') ('are', 'now') ('now', 'joined') ('joined', 'in') ('in', 'a') ('a', 'great') ('great', 'national') ('national', 'effort') ('effort', 'to') ('to', 'rebuild') ('rebuild', 'our') ('our', 'country') ('country', 'and') ('and', 'restore') ('restore', 'its') ('its', 'promise') ('promise', 'for') ('for', 'all') ('all', 'of') ('of', 'our') ('our', 'people.') ('people.', 'Together,') ('Together,', 'we') ('we', 'will') ('will', 'determine') ('determine', 'the') ('the', 'course') ('course', 'of') ('of', 'America') ('America', 'and') ('and', 'the') ('the', 'world') ('world', 'for') ('for', 'many,') ('many,', 'many') ('many', 'years') ('years', 'to') ('to', 'come.') ('come.', 'We') ('We', 'will') ('will', 'face') ('face', 'challenges.') ('challenges.', 'We') ('We', 'will') ('will', 'confront') ('confront', 'hardships,') ('hardships,', 'but') ('but', 'we') ('we', 'will') ('will', 'get') ('get', 'the') ('the', 'job') ('job', 'done.') "
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"Chief Justice Roberts, President Cater, President Clinton, President Bush, President Obama, fellow Americans and people of the world, thank you. We, the citizens of America are now joined in a great national effort to rebuild our country and restore its promise for all of our people. Together, we will determine the course of America and the world for many, many years to come. We will face challenges. We will confront hardships, but we will get the job done.\"\n",
    "grams = ngrams(sentence.split(), 2)\n",
    "\n",
    "for gram in grams:\n",
    "    print(gram, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 품사분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "품사분석 = POS(Part-Of-Speech)태깅  \n",
    "  \n",
    "품사분석의 목적은 모든 언어에 공통적으로 존재하는 품사인 명사, 동사, 형용사, 부사를 살피는 것이다.  \n",
    "  \n",
    "한국어 품사 분석은 `KoNLPy` 패키지 이용한다. 해당 패키지 내의 `Kkma`, `Komoran`, `Hannanum`, `Twitter`, `Mecab` 클래스를 연구자가 원하는 대로 선택하여 이용한다. 선택 기준은 `실행 시간`과 `정확도`이다.  \n",
    "\n",
    "> `Twitter`는 클래스명이 `konlpy v0.4.5`부터 `Okt`로 바뀌었다.\n",
    "  \n",
    "클래스마다 정의하는 품사와 태깅 기호가 다르므로 사전에 숙지해야한다.  \n",
    "  \n",
    "< 유용한 매서드 >  \n",
    ">``morphs`` : 문장을 형태소 단위로 끊는다.  \n",
    "``nouns`` : 문장에서 명사만을 추출한다.  \n",
    "``POS`` : 문장에 속한 각 형태소에 품사를 태깅한다.\n",
    "``Phrases`` : Twitter클래스의 매서드. 문장을 구 단위로 쪼갠다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척들', '이', '모이', 'ㄴ', '이번', '추석', '차례상', '에서는', '단연', \"'\", '집값', \"'\", '이', '화제', '에', '오르', '아다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "\n",
    "print(hannanum.morphs(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척들', '이번', '추석', '차례상', '집값', '화제']\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.nouns(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친척들', 'N'), ('이', 'J'), ('모이', 'P'), ('ㄴ', 'E'), ('이번', 'N'), ('추석', 'N'), ('차례상', 'N'), ('에서는', 'J'), ('단연', 'M'), (\"'\", 'S'), ('집값', 'N'), (\"'\", 'S'), ('이', 'J'), ('화제', 'N'), ('에', 'J'), ('오르', 'P'), ('아다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.pos(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\", ntags=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친척들', 'NC'), ('이', 'JC'), ('모이', 'PV'), ('ㄴ', 'ET'), ('이', 'NN'), ('번', 'NB'), ('추석', 'NC'), ('차례상', 'NC'), ('에서', 'JC'), ('는', 'JX'), ('단연', 'MA'), (\"'\", 'SR'), ('집값', 'NC'), (\"'\", 'SR'), ('이', 'JC'), ('화제', 'NC'), ('에', 'JC'), ('오르', 'PV'), ('아', 'EP'), ('다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.pos(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\", ntags=22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척', '들', '이', '모이', 'ㄴ', '이번', '추석', '차례', '상', '에서', '는', '단연', \"'\", '집', '값', \"'\", '이', '화제', '에', '오르', '았', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "print(kkma.morphs(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척', '이번', '추석', '차례', '차례상', '상', '집', '집값', '값', '화제']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친척', 'NNG'), ('들', 'XSN'), ('이', 'JKS'), ('모이', 'VV'), ('ㄴ', 'ETD'), ('이번', 'NNG'), ('추석', 'NNG'), ('차례', 'NNG'), ('상', 'NNG'), ('에서', 'JKM'), ('는', 'JX'), ('단연', 'MAG'), (\"'\", 'SS'), ('집', 'NNG'), ('값', 'NNG'), (\"'\", 'SS'), ('이', 'MDT'), ('화제', 'NNG'), ('에', 'JKM'), ('오르', 'VV'), ('았', 'EPT'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척', '들', '이', '모인', '이번', '추석', '차례상', '에서는', '단연', \"'\", '집값', \"'\", '이', '화제', '에', '올랐다', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.morphs(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척', '이번', '추석', '차례상', '단연', '집값', '이', '화제']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('친척', 'Noun'), ('들', 'Suffix'), ('이', 'Josa'), ('모인', 'Verb'), ('이번', 'Noun'), ('추석', 'Noun'), ('차례상', 'Noun'), ('에서는', 'Josa'), ('단연', 'Noun'), (\"'\", 'Punctuation'), ('집값', 'Noun'), (\"'\", 'Punctuation'), ('이', 'Noun'), ('화제', 'Noun'), ('에', 'Josa'), ('올랐다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친척들', '이번', '이번 추석', '이번 추석 차례상', '단연', '집값', '이 화제', '추석', '차례상', '화제']\n"
     ]
    }
   ],
   "source": [
    "print(okt.phrases(\"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "영어 품사분석은 `NLTK` 패키지를 이용하여 수행할 수 있다.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('Persian', 'NNP'), ('cat.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = \"The little yellow dog barked at the Persian cat.\".split()\n",
    "tags_en = pos_tag(tokens)\n",
    "print(tags_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
